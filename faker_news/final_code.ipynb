{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "匹配完成，结果已输出到 combine_news.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 主表A的路径\n",
    "main_table_path = 'combine_news.xlsx'  # 替换为主表A的实际路径\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = \"D:/mycodelife/workshop/fake_finance/course_task\"  # 替换为文件夹的实际路径\n",
    "\n",
    "# 输出总表路径\n",
    "output_path = 'combine_news.xlsx'  # 替换为输出文件的实际路径\n",
    "\n",
    "# 读取主表A\n",
    "df_main = pd.read_excel(main_table_path,sheet_name= 0 ,dtype={'股票代码':str})\n",
    "\n",
    "# 确保主码列名正确\n",
    "main_keys = ['股票代码', '新闻发布时间']\n",
    "\n",
    "# 检查主表是否包含主码列\n",
    "if not set(main_keys).issubset(df_main.columns):\n",
    "    raise ValueError(f\"主表 A 缺少主码列：{set(main_keys) - set(df_main.columns)}\")\n",
    "\n",
    "# 初始化结果表\n",
    "result = df_main.copy()\n",
    "\n",
    "# 遍历文件夹中的所有 Excel 文件\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx') or file_name.endswith('.xls'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # 读取当前文件\n",
    "        df = pd.read_excel(file_path,dtype={'股票代码':str})\n",
    "\n",
    "        # 检查当前表是否包含主码列\n",
    "        if not set(main_keys).issubset(df.columns):\n",
    "            print(f\"文件 {file_name} 缺少主码列，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        # 按主码合并\n",
    "        result = pd.merge(result, df, on=main_keys, how='left', suffixes=('', f'_{file_name}'))\n",
    "\n",
    "# 将结果输出到总表\n",
    "result.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"匹配完成，结果已输出到 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86198\\AppData\\Local\\Temp\\ipykernel_18540\\3893153755.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_related[col] = data_related.apply(lambda row: fill_with_nearest(row, col, neighbors), axis=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data  = pd.read_excel('combine_news.xlsx',dtype={'股票代码':str})\n",
    "# Define the columns of interest for titles and comments\n",
    "title_cols = [\"标题中性分数 (t+0)\", \"标题消极分数 (t+0)\", \"标题积极分数 (t+0)\"]\n",
    "comment_cols = [\"评论中性分数 (t+0)\", \"评论消极分数 (t+0)\", \"评论积极分数 (t+0)\"]\n",
    "\n",
    "# Extract the columns of interest along with neighboring ones for filling\n",
    "related_cols = [\n",
    "    \"标题中性分数 (t+0)\", \"标题消极分数 (t+0)\", \"标题积极分数 (t+0)\",\n",
    "    \"标题中性分数 (t+1)\", \"标题消极分数 (t+1)\", \"标题积极分数 (t+1)\",\n",
    "    \"标题中性分数 (t-1)\", \"标题消极分数 (t-1)\", \"标题积极分数 (t-1)\",\n",
    "    \"评论中性分数 (t+0)\", \"评论消极分数 (t+0)\", \"评论积极分数 (t+0)\",\n",
    "    \"评论中性分数 (t+1)\", \"评论消极分数 (t+1)\", \"评论积极分数 (t+1)\",\n",
    "    \"评论中性分数 (t-1)\", \"评论消极分数 (t-1)\", \"评论积极分数 (t-1)\",\n",
    "]\n",
    "\n",
    "# Subset the related columns\n",
    "data_related = data[related_cols]\n",
    "\n",
    "# Fill missing values for each column of interest\n",
    "def fill_with_nearest(row, main_col, neighbors):\n",
    "    if not np.isnan(row[main_col]):\n",
    "        return row[main_col]\n",
    "    for neighbor in neighbors:\n",
    "        if not np.isnan(row[neighbor]):\n",
    "            return row[neighbor]\n",
    "    return np.nan\n",
    "\n",
    "# Columns for which we need to find neighbors\n",
    "title_neighbors = {\n",
    "    \"标题中性分数 (t+0)\": [\"标题中性分数 (t+1)\", \"标题中性分数 (t-1)\"],\n",
    "    \"标题消极分数 (t+0)\": [\"标题消极分数 (t+1)\", \"标题消极分数 (t-1)\"],\n",
    "    \"标题积极分数 (t+0)\": [\"标题积极分数 (t+1)\", \"标题积极分数 (t-1)\"],\n",
    "}\n",
    "comment_neighbors = {\n",
    "    \"评论中性分数 (t+0)\": [\"评论中性分数 (t+1)\", \"评论中性分数 (t-1)\"],\n",
    "    \"评论消极分数 (t+0)\": [\"评论消极分数 (t+1)\", \"评论消极分数 (t-1)\"],\n",
    "    \"评论积极分数 (t+0)\": [\"评论积极分数 (t+1)\", \"评论积极分数 (t-1)\"],\n",
    "}\n",
    "\n",
    "# Apply the filling logic\n",
    "for col, neighbors in {**title_neighbors, **comment_neighbors}.items():\n",
    "    data_related[col] = data_related.apply(lambda row: fill_with_nearest(row, col, neighbors), axis=1)\n",
    "\n",
    "# Combine the filled columns with the original data\n",
    "result_data = pd.concat([data.drop(columns=related_cols), data_related[title_cols + comment_cols]], axis=1)\n",
    "\n",
    "# Show the first few rows of the processed data\n",
    "result_data.head()\n",
    "\n",
    "result_data.to_excel(\"ml_without_label.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: 读取Excel文件\n",
    "file_path = \"combine_news.xlsx\"  # 替换为您的文件路径\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: 填充逻辑\n",
    "# 定义需要处理的列及邻近列范围\n",
    "new_title_neighbors = {\n",
    "    \"标题中性分数 (t+0)\": [f\"标题中性分数 (t{n:+d})\" for n in range(-4, 4) if n != 0],\n",
    "    \"标题消极分数 (t+0)\": [f\"标题消极分数 (t{n:+d})\" for n in range(-4, 4) if n != 0],\n",
    "    \"标题积极分数 (t+0)\": [f\"标题积极分数 (t{n:+d})\" for n in range(-4, 4) if n != 0],\n",
    "}\n",
    "new_comment_neighbors = {\n",
    "    \"评论中性分数 (t+0)\": [f\"评论中性分数 (t{n:+d})\" for n in range(-4, 4) if n != 0],\n",
    "    \"评论消极分数 (t+0)\": [f\"评论消极分数 (t{n:+d})\" for n in range(-4, 4) if n != 0],\n",
    "    \"评论积极分数 (t+0)\": [f\"评论积极分数 (t{n:+d})\" for n in range(-4, 4) if n != 0],\n",
    "}\n",
    "\n",
    "# 定义填充函数：如果目标列为空，依次从邻近列寻找非空值填充\n",
    "def fill_with_nearest(row, main_col, neighbors):\n",
    "    if not np.isnan(row[main_col]):\n",
    "        return row[main_col]\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor in row and not np.isnan(row[neighbor]):\n",
    "            return row[neighbor]\n",
    "    return np.nan\n",
    "\n",
    "# 对每个t+0列进行填充处理\n",
    "for col, neighbors in {**new_title_neighbors, **new_comment_neighbors}.items():\n",
    "    data[col] = data.apply(lambda row: fill_with_nearest(row, col, neighbors), axis=1)\n",
    "\n",
    "# Step 3: 增加标签\n",
    "# 计算每个(t+0)列的均值（忽略NaN），生成标签列\n",
    "for col in [\n",
    "    \"标题中性分数 (t+0)\", \"标题消极分数 (t+0)\", \"标题积极分数 (t+0)\",\n",
    "    \"评论中性分数 (t+0)\", \"评论消极分数 (t+0)\", \"评论积极分数 (t+0)\"\n",
    "]:\n",
    "    mean_value = data[col].mean(skipna=True)\n",
    "    data[f\"{col} 标签\"] = data[col].apply(lambda x: 1 if x > mean_value else (0 if not np.isnan(x) else np.nan))\n",
    "\n",
    "# Step 4: 输出最终数据\n",
    "# 保留所有非 (t+n) 相关列，以及处理后的 t+0 列和其标签\n",
    "t0_columns = [\n",
    "    \"标题中性分数 (t+0)\", \"标题消极分数 (t+0)\", \"标题积极分数 (t+0)\",\n",
    "    \"评论中性分数 (t+0)\", \"评论消极分数 (t+0)\", \"评论积极分数 (t+0)\"\n",
    "]\n",
    "label_columns = [f\"{col} 标签\" for col in t0_columns]\n",
    "non_tn_columns = [col for col in data.columns if not any(suffix in col for suffix in [\"(t+\", \"(t-\", \"(t0)\"])]\n",
    "\n",
    "final_columns = non_tn_columns + t0_columns + label_columns\n",
    "final_data = data[final_columns]\n",
    "\n",
    "# Step 5: 保存到Excel文件\n",
    "output_file_path = \"processed_with_labels.xlsx\"  # 替换为您想保存的路径\n",
    "final_data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果评论中性分数为空，则用新闻真实性填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取处理好的数据\n",
    "file_path = \"processed_with_labels.xlsx\"  # 替换为您的文件路径\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: 填充缺失值\n",
    "# 如果“评论中性分数 (t+0) 标签”列中有缺失值，用“真实性（虚假1真实0）”的值填充\n",
    "data[\"评论中性分数 (t+0) 标签\"] = data[\"评论中性分数 (t+0) 标签\"].fillna(data[\"真实性（虚假1真实0）\"])\n",
    "\n",
    "# Step 3: 保存处理后的数据\n",
    "output_file_path = \"filled_labels.xlsx\"  # 替换为您想保存的路径\n",
    "data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除列“新闻文本情绪”之类都是空的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "file_path = \"filled_labels.xlsx\"  # 替换为您的文件路径\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: 删除特定列全为空的行\n",
    "# 指定需要检查的列\n",
    "columns_to_check = [\"新闻内容中性分数\", \"新闻内容积极分数\", \"新闻内容消极分数\"]\n",
    "\n",
    "# 删除所有指定列都为空的行\n",
    "data = data.dropna(subset=columns_to_check, how='all')\n",
    "\n",
    "# Step 3: 保存处理后的数据\n",
    "output_file_path = \"filtered_data.xlsx\"  # 替换为您想保存的路径\n",
    "data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将日期转化为数值，将来源分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "file_path = \"filtered_data.xlsx\"  # 替换为您的文件路径\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: 处理“新闻实际来源”\n",
    "# 统计“新闻实际来源”的唯一值\n",
    "unique_sources = data[\"新闻实际来源\"].dropna().unique()\n",
    "\n",
    "# 创建来源类别映射表\n",
    "source_mapping = {source: idx for idx, source in enumerate(unique_sources)}\n",
    "\n",
    "# 新增“来源类别”列\n",
    "data[\"来源类别\"] = data[\"新闻实际来源\"].map(source_mapping)\n",
    "\n",
    "# Step 3: 处理“新闻发布时间”\n",
    "# 转换为日期格式\n",
    "data[\"新闻发布时间\"] = pd.to_datetime(data[\"新闻发布时间\"])\n",
    "\n",
    "# 找到最早的日期并设为基准\n",
    "earliest_date = data[\"新闻发布时间\"].min()\n",
    "\n",
    "# 新增“发布时间”列，计算日期偏移（以天为单位）\n",
    "data[\"发布时间\"] = (data[\"新闻发布时间\"] - earliest_date).dt.days\n",
    "\n",
    "# Step 4: 保存处理后的数据\n",
    "output_file_path = \"processed_data.xlsx\"  # 替换为您想保存的路径\n",
    "data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充基本面数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "file_path = \"processed_data.xlsx\"  # 替换为您的文件路径\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: 获取第 H 到第 M 列\n",
    "# 假设 H 到 M 是列索引的位置，例如 H = 7, M = 12（以 0 为起始索引）\n",
    "columns_to_process = data.columns[7:13]  # 替换为实际的 H 到 M 列范围\n",
    "\n",
    "# Step 3: 填充空值为非空值的平均值\n",
    "for column in columns_to_process:\n",
    "    mean_value = data[column].mean(skipna=True)  # 计算非空值的平均值\n",
    "    data[column].fillna(mean_value, inplace=True)  # 用平均值填充空值\n",
    "\n",
    "# Step 4: 保存处理后的数据\n",
    "output_file_path = \"filled_data.xlsx\"  # 替换为您想保存的路径\n",
    "data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sources = data[\"新闻标题情绪标签\"].dropna().unique()\n",
    "\n",
    "# 创建来源类别映射表\n",
    "source_mapping = {source: idx for idx, source in enumerate(unique_sources)}\n",
    "\n",
    "# 新增“来源类别”列\n",
    "data[\"新闻标题类别\"] = data[\"新闻标题情绪标签\"].map(source_mapping)\n",
    "output_file_path = \"filled_data.xlsx\"  # 替换为您想保存的路径\n",
    "data.to_excel(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
