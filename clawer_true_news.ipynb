{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import typing\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, date\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from DrissionPage import ChromiumOptions, ChromiumPage\n",
    "from DrissionPage.common import Settings\n",
    "from DrissionPage.errors import ElementNotFoundError, WaitTimeoutError\n",
    "from rich import print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取原始 Excel 文件\n",
    "input_file = \"data_china.xlsx\"  # 输入文件名\n",
    "output_file = \"group_control_prim.xlsx\"  # 输出文件名\n",
    "\n",
    "# 读取 Excel 数据\n",
    "df = pd.read_excel(input_file,sheet_name=0,dtype={'股票代码': str})\n",
    "\n",
    "# 筛选出 \"真实性\" 列值为 0 的行\n",
    "filtered_df = df[df[\"真实性（虚假1真实0）\"] == 0]\n",
    "\n",
    "# 选择保留的列\n",
    "columns_to_keep = [\"股票代码\", \"公司简称\", \"新闻发布时间\"]\n",
    "filtered_df = filtered_df[columns_to_keep]\n",
    "\n",
    "filtered_df[\"新闻发布时间\"] = pd.to_datetime(filtered_df[\"新闻发布时间\"]).dt.date\n",
    "# 将筛选后的数据保存到新的 Excel 文件\n",
    "filtered_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"已成功保存筛选后的数据到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未爬取的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = \"D:\\mycodelife\\workshop\\\\fake_finance\\\\ready_crawler\"\n",
    "\n",
    "# 获取文件夹中所有文件的文件名\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# 假设文件名即为股票代码，去掉扩展名并存储为集合\n",
    "crawled_codes = {os.path.splitext(file)[0] for file in files}\n",
    "\n",
    "df = pd.read_excel(\"group_control_prim.xlsx\",dtype={'股票代码': str})\n",
    "# 股票代码列表（可以从一个 DataFrame 中读取，例如）\n",
    "all_codes = df[\"股票代码\"]  # 示例股票代码\n",
    "\n",
    "# 找出未爬取的股票代码并输出\n",
    "uncrawled_df = df[~df[\"股票代码\"].isin(crawled_codes)]\n",
    "\n",
    "# 保存未爬取股票代码的信息到新文件\n",
    "output_file = \"uncrawled_codes.xlsx\"\n",
    "uncrawled_df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未爬取的股票代码及新闻发布时间已保存到: uncrawled_codes_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = r\"D:\\\\mycodelife\\workshop\\\\fake_finance\\\\codes_new\\\\already_done\"\n",
    "\n",
    "# 获取文件夹中所有文件的文件名\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# 假设文件名格式为 \"股票代码_新闻发布时间.csv\"\n",
    "# 提取主码（股票代码_新闻发布时间），并存储为集合\n",
    "crawled_codes = {os.path.splitext(file)[0] for file in files}\n",
    "\n",
    "# 读取 Excel 文件\n",
    "df = pd.read_excel(\"D:\\mycodelife\\workshop\\\\fake_finance\\\\faker_news\\data_china.xlsx\", sheet_name= 1, dtype={'股票代码': str})\n",
    "df[\"新闻发布时间\"] = pd.to_datetime(df[\"新闻发布时间\"]).dt.date  # 确保日期格式一致\n",
    "df[\"主码\"] = df[\"股票代码\"] + \"_\" + df[\"新闻发布时间\"].astype(str)  # 生成主码\n",
    "\n",
    "# 找出未爬取的主码\n",
    "uncrawled_df = df[~df[\"主码\"].isin(crawled_codes)]\n",
    "\n",
    "# 保存未爬取股票代码及其发布时间的信息到新文件\n",
    "output_file = \"uncrawled_codes_test.xlsx\"\n",
    "uncrawled_df[[\"股票代码\", \"新闻发布时间\"]].to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"未爬取的股票代码及新闻发布时间已保存到: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"uncrawled_codes.xlsx\",dtype={'股票代码':str})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 300587 on 2021-04-27...\n",
      "https://guba.eastmoney.com/list,300587_1.html\n",
      "Total page: 331\n",
      "目标页面: 176\n",
      "Fetching 002331 on 2021-04-28...\n",
      "https://guba.eastmoney.com/list,002331_1.html\n",
      "Total page: 780\n",
      "目标页面: 262\n",
      "Fetching 688101 on 2021-04-30...\n",
      "https://guba.eastmoney.com/list,688101_1.html\n",
      "Total page: 183\n",
      "目标页面: 96\n",
      "Fetching 600418 on 2021-07-27...\n",
      "https://guba.eastmoney.com/list,600418_1.html\n",
      "Total page: 4876\n",
      "目标页面: 2289\n",
      "Fetching 601012 on 2022-03-01...\n",
      "https://guba.eastmoney.com/list,601012_1.html\n",
      "Total page: 6586\n",
      "目标页面: 3498\n",
      "Fetching 601012 on 2022-04-21...\n",
      "https://guba.eastmoney.com/list,601012_1.html\n",
      "Total page: 6586\n",
      "目标页面: 3498\n",
      "Fetching 603023 on 2022-06-24...\n",
      "https://guba.eastmoney.com/list,603023_1.html\n",
      "Total page: 682\n",
      "目标页面: 250\n",
      "Fetching 000980 on 2022-11-04...\n",
      "https://guba.eastmoney.com/list,000980_1.html\n",
      "Total page: 11466\n",
      "目标页面: 3796\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import typing\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, date\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from DrissionPage import ChromiumOptions, ChromiumPage\n",
    "from DrissionPage.common import Settings\n",
    "from DrissionPage.errors import ElementNotFoundError, WaitTimeoutError\n",
    "\n",
    "DEBUG = False\n",
    "DT_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "Settings.raise_when_wait_failed = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Article:\n",
    "    title: str\n",
    "    url: str\n",
    "    published: datetime\n",
    "    content: str\n",
    "\n",
    "\n",
    "class PageNotFound(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_chrome_options() -> ChromiumOptions:\n",
    "    co = ChromiumOptions()\n",
    "    co.set_argument(\"--no-sandbox\")\n",
    "    if not DEBUG:\n",
    "        co.headless()\n",
    "        co.set_user_agent(\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    return co\n",
    "\n",
    "\n",
    "class Fetcher:\n",
    "    def __init__(self, code: str, date: date, page: ChromiumPage):\n",
    "        self.code = code\n",
    "        self.start_url = f\"https://guba.eastmoney.com/list,{code}\"\n",
    "        self.date = date\n",
    "        self.page = page\n",
    "        self.total_page = -1\n",
    "\n",
    "    def get_url(self, pn: int) -> str:\n",
    "        return f\"{self.start_url}_{pn}.html\"\n",
    "\n",
    "    def get_article_url(self, post_id: int) -> str:\n",
    "        return f\"https://guba.eastmoney.com/news,{self.code},{post_id}.html\"\n",
    "\n",
    "    def get_article_list(self, pn: int) -> typing.Generator[Article, None, None]:\n",
    "        self.page.get(self.get_url(pn))\n",
    "        #print(f\"[green] Fetching page {self.get_url(pn)}...\")\n",
    "        article_list = re.search(\n",
    "            r\"var article_list\\s*=\\s*({.*?});\", self.page.html\n",
    "        ).group(1)\n",
    "\n",
    "        json_data = json.loads(article_list)\n",
    "        articles = json_data[\"re\"]\n",
    "        for article in articles:\n",
    "            yield Article(\n",
    "                title=article[\"post_title\"],\n",
    "                url=self.get_article_url(article[\"post_id\"]),\n",
    "                published=datetime.strptime(article[\"post_publish_time\"], DT_FMT),\n",
    "                content=\"\",\n",
    "            )\n",
    "\n",
    "    def get_total_page(self):\n",
    "        if self.total_page == -1:\n",
    "            print(self.get_url(1))\n",
    "            self.page.get(self.get_url(1), retry=3)\n",
    "            if self.page.url == \"https://guba.eastmoney.com/error?type=1\":\n",
    "                raise PageNotFound(\"Page not found\")\n",
    "            self.page.wait.ele_displayed(\"t:ul@class:paging\")\n",
    "            pagers = self.page.ele(\"t:ul@class:paging\").eles(\"t:li\")\n",
    "            last_page = pagers[-2].text\n",
    "            self.total_page = int(last_page)\n",
    "            print(f\"Total page: {self.total_page}\")\n",
    "\n",
    "    def get_article_detail(self, article: Article) -> Article:\n",
    "        # print(f\"[green] {article.published} {article.title}\")\n",
    "        new_tab = self.page.new_tab()\n",
    "        try:\n",
    "            new_tab.get(article.url)\n",
    "            new_tab.wait.ele_displayed(\"@class:newstext\")\n",
    "            article.content = new_tab.ele(\"@class:newstext\").text\n",
    "            return article\n",
    "        except:\n",
    "            return article\n",
    "        finally:\n",
    "            new_tab.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_range(articles: typing.Iterable[Article]) -> tuple[date, date]:\n",
    "        articles = list(articles)\n",
    "\n",
    "        return articles[0].published.date(), articles[-1].published.date()\n",
    "\n",
    "    def get_articles_with_date(self) -> typing.Generator[Article, None, None]:\n",
    "        self.get_total_page()\n",
    "\n",
    "        # Binary search\n",
    "        start, end = 1, self.total_page\n",
    "\n",
    "        while start <= end:\n",
    "            # print(f\"[blue] Searching: {start} - {end}\")\n",
    "            mid = (start + end) // 2\n",
    "            earliest_date_mid, latest_date_mid = self.get_date_range(\n",
    "                self.get_article_list(mid)\n",
    "            )\n",
    "\n",
    "            if self.date > earliest_date_mid:\n",
    "                end = mid - 1\n",
    "            elif self.date < latest_date_mid:\n",
    "                start = mid + 1\n",
    "            else:\n",
    "                end = mid - 1\n",
    "\n",
    "        print(f\"目标页面: {start}\")\n",
    "\n",
    "        # 获取目标页前后3天的文章\n",
    "        start_page = start - 1 if start - 1 > 0 else 1\n",
    "        end_page = end + 1 if end + 1 <= self.total_page else self.total_page\n",
    "\n",
    "        # 未做优化，maxworker过大会爆内存\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            for page in range(start_page, end_page + 1):\n",
    "                # print(f\"[blue] Fetching page {page}...\")\n",
    "                yield from executor.map(\n",
    "                    self.get_article_detail, self.get_article_list(page)\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import csv\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    wb = load_workbook(\"uncrawled_codes_test.xlsx\")\n",
    "    ws = wb.active\n",
    "    chrome_page = ChromiumPage(get_chrome_options())\n",
    "    for row in ws.iter_rows(min_row=2, values_only=True):\n",
    "        code, dt = row\n",
    "        dt = dt.date()\n",
    "\n",
    "        output = f\"{code}_{dt}.csv\"\n",
    "        print(f\"Fetching {code} on {dt}...\")\n",
    "        fetcher = Fetcher(code, dt, chrome_page)\n",
    "\n",
    "        with open(output, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"标题\", \"链接\", \"发布时间\", \"内容\"])\n",
    "            try:\n",
    "                for article in fetcher.get_articles_with_date():\n",
    "                    writer.writerow(\n",
    "                        [article.title, article.url, article.published, article.content]\n",
    "                    )\n",
    "                   \n",
    "            except PageNotFound:\n",
    "                print(f\"Page not found: {code}\")\n",
    "            except ElementNotFoundError:\n",
    "                print(f\"Element not found: {code}\")\n",
    "            except WaitTimeoutError:\n",
    "                print(f\"Wait timeout: {code}\")\n",
    "\n",
    "        # fetcher.page.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
